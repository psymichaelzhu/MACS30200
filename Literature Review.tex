\documentclass[man]{apa7}
\usepackage[american]{babel}
\usepackage{csquotes}

\title{Automated Representation Discovery in Neural Encoding of Social Interaction}
\author{}
\affiliation{}

\begin{document}

\section{Introduction}

In everyday life, we constantly observe others engaging in social interactions. These interactions are inherently multi-faceted: What is the relationship between the individuals involved? What activity are they engaged in? Is the emotional tone positive or negative? In what context does the interaction occur? Are the individuals cooperating toward a shared goal or pursuing independent objectives? Each of these dimensions provides a potential cue for interpreting social meaning. What, then, is represented in the brain when we view these complex and dynamic social interactions?

In social psychology and neuroscience, understanding how humans represent social information has long been a central question, often focusing on identifying key underlying dimensions. For example, face perception studies have shown that trait judgments such as trustworthiness, competence, and warmth can be mapped onto low-dimensional spaces that efficiently capture social evaluations across individuals \cite{todorov_social_2015}. Recent work further reveals that multiple social attributes—including race, gender, age, and emotional expression—are simultaneously encoded in dynamic and interacting neural representations, reflecting the multifaceted and temporally structured nature of social perception \cite{xu_decoding_2024}. Theoretical frameworks similarly propose that people organize social knowledge across multiple layers—traits (Goodwin et al., 2014), mental states (Tamir et al., 2016), actions (Thornton et al., 2019), and relationships (Cheng et al., 2025)—each structured along a small set of latent psychological dimensions. Together, these findings suggest that social life, despite its complexity, can be efficiently represented within condensed cognitive spaces (Tamir & Thornton, 2018).

Focusing on social interaction representation specifically, recent work suggests that social interactions are processed hierarchically along the lateral visual pathway (McMahon & Isik, 2023), beginning with low-level cues such as agent distance and progressing to more abstract features such as communication, engaging regions from the extrastriate body area (EBA) to the posterior superior temporal sulcus (pSTS). Complementary evidence from naturalistic movie-viewing paradigms shows that regions such as the superior temporal gyrus (STG) and middle temporal gyrus (MTG) encode high-level social perceptual features, while the temporoparietal junction (TPJ) selectively represents information related to mentalizing (Lee Masson et al., 2024).

Although these findings have significantly advanced our understanding, a systematic account of the key dimensions underlying social interaction perception remains elusive. Developing generalizable theoretical frameworks requires testing a broad range of hypotheses simultaneously on the same set of stimuli and participants, thereby eliminating confounding differences across studies (Almaatouq et al., 2024; Awad et al., 2018; Peterson et al., 2021). A major limitation of prior work is its reliance on a narrow set of predefined features, restricting exploration of the broader conceptual space. Fundamentally, this stems from a heavy dependence on human annotations, which limits the dimensionality of hypothesis testing and constrains the ability to fully capture the richness of naturalistic social stimuli.

To address these challenges, there has been a growing call to shift toward naturalistic computational cognitive science, integrating ecologically valid stimuli and computational models to uncover generalizable mechanisms (Carvalho & Lampinen, 2025). Emerging resources such as the Moments in Time dataset (Monfort et al., 2019) provide a rich and diverse testbed for this inquiry. In parallel, computational tools like deep neural networks (DNNs) offer scalable tools for modeling high-dimensional social information and decoding neural representations (Khaligh-Razavi & Kriegeskorte, 2014; Güçlü & van Gerven, 2015; Bao et al., 2023). However, DNN-based approaches often suffer from limited interpretability, making it difficult to map learned features onto meaningful psychological dimensions (Sievers & Thornton, 2024).

Exemplar-based approaches offer an alternative. Rather than describing stimuli through numerical features, archetypal analysis identifies a set of representative exemplars that anchor the conceptual space (Cutler & Breiman, 1994; Fel et al., 2025). Recent findings suggest that exemplar-based models may better explain neural activity associated with social relationship perception than dimension-based models, highlighting their potential to offer a more psychologically and neurally meaningful basis for understanding social cognition. 

Recent advances in embedding projection techniques and multi-modal models provide a promising pathway for implementing exemplar annotation of complex, naturalistic stimuli. Embedding projection, which follows the same underlying idea as exemplar-based frameworks, organizes stimuli by their proximity to a structured set of reference exemplars rather than along arbitrary continuous axes. Multi-modal models such as Contrastive Language-Image Pretraining (CLIP; Radford et al., 2021), which align visual and linguistic representations into a shared embedding space, enable automated and interpretable identification of representative exemplars across a large conceptual space. 

Drawing on these insights, the present project proposes a scalable and interpretable framework to systematically investigate the neural representation of social interactions. We utilize a naturalistic fMRI dataset in which participants viewed 244 short video clips of dyadic social interactions, selected from the Moments in Time dataset. To annotate these complex stimuli, we employ CLIP model and embedding projection to automatically generate structured, hypothesis-based annotations across key conceptual dimensions, such as relationship, activity, and context. Rather than relying on raw feature embeddings, we operationalize each conceptual hypothesis into a structured set of exemplars, enabling fine-grained and interpretable annotation of video content. We then apply representational similarity analysis (RSA) to compare these hypothesis-driven annotation structures with fMRI response patterns across social perceptual regions such as pSTS, STG, and TPJ. This integrated approach allows us to systematically test competing hypotheses of how social interactions are represented in the brain.


\end{document}
