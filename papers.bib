
@article{peterson_using_2021,
	title = {Using large-scale experiments and machine learning to discover theories of human decision-making},
	volume = {372},
	url = {https://www.science.org/doi/10.1126/science.abe2629},
	doi = {10.1126/science.abe2629},
	abstract = {Predicting and understanding how people make decisions has been a long-standing goal in many fields, with quantitative models of human decision-making informing research in both the social sciences and engineering. We show how progress toward this goal can be accelerated by using large datasets to power machine-learning algorithms that are constrained to produce interpretable psychological theories. Conducting the largest experiment on risky choice to date and analyzing the results using gradient-based optimization of differentiable decision theories implemented through artificial neural networks, we were able to recapitulate historical discoveries, establish that there is room to improve on existing theories, and discover a new, more accurate model of human decision-making in a form that preserves the insights from centuries of research.},
	pages = {1209--1214},
	number = {6547},
	journaltitle = {Science},
	author = {Peterson, Joshua C. and Bourgin, David D. and Agrawal, Mayank and Reichman, Daniel and Griffiths, Thomas L.},
	urldate = {2025-04-28},
	date = {2021-06-11},
	langid = {american},
	note = {Publisher: American Association for the Advancement of Science},
}

@article{thornton_theories_2018,
	title = {Theories of Person Perception Predict Patterns of Neural Activity During Mentalizing},
	volume = {28},
	issn = {1047-3211},
	url = {https://doi.org/10.1093/cercor/bhx216},
	doi = {10.1093/cercor/bhx216},
	abstract = {Social life requires making inferences about other people. What information do perceivers spontaneously draw upon to make such inferences? Here, we test 4 major theories of person perception, and 1 synthetic theory that combines their features, to determine whether the dimensions of such theories can serve as bases for describing patterns of neural activity during mentalizing. While undergoing functional magnetic resonance imaging, participants made social judgments about well-known public figures. Patterns of brain activity were then predicted using feature encoding models that represented target people's positions on theoretical dimensions such as warmth and competence. All 5 theories of person perception proved highly accurate at reconstructing activity patterns, indicating that each could describe the informational basis of mentalizing. Cross-validation indicated that the theories robustly generalized across both targets and participants. The synthetic theory consistently attained the best performance—approximately two-thirds of noise ceiling accuracy––indicating that, in combination, the theories considered here can account for much of the neural representation of other people. Moreover, encoding models trained on the present data could reconstruct patterns of activity associated with mental state representations in independent data, suggesting the use of a common neural code to represent others’ traits and states.},
	pages = {3505--3520},
	number = {10},
	journaltitle = {Cerebral Cortex},
	shortjournal = {Cerebral Cortex},
	author = {Thornton, Mark A and Mitchell, Jason P},
	urldate = {2025-04-28},
	date = {2018-10-01},
}

@article{tamir_modeling_2018,
	title = {Modeling the Predictive Social Mind},
	volume = {22},
	issn = {1364-6613, 1879-307X},
	url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(17)30267-X},
	doi = {10.1016/j.tics.2017.12.005},
	pages = {201--212},
	number = {3},
	journaltitle = {Trends in Cognitive Sciences},
	shortjournal = {Trends in Cognitive Sciences},
	author = {Tamir, Diana I. and Thornton, Mark A.},
	urldate = {2025-04-28},
	date = {2018-03-01},
	pmid = {29361382},
	note = {Publisher: Elsevier},
}

@article{biderman_role_2023,
	title = {The role of memory in counterfactual valuation.},
	volume = {152},
	rights = {http://www.apa.org/pubs/journals/resources/open-access.aspx},
	issn = {1939-2222, 0096-3445},
	url = {https://doi.apa.org/doi/10.1037/xge0001364},
	doi = {10.1037/xge0001364},
	pages = {1754--1767},
	number = {6},
	journaltitle = {Journal of Experimental Psychology: General},
	shortjournal = {Journal of Experimental Psychology: General},
	author = {Biderman, Natalie and Gershman, Samuel J. and Shohamy, Daphna},
	urldate = {2025-04-21},
	date = {2023-06},
	langid = {english},
}

@article{lopez-perez_exploring_2025,
	title = {Exploring the potential of large language models to understand interpersonal emotion regulation strategies from narratives},
	issn = {1931-1516},
	doi = {10.1037/emo0001528},
	abstract = {Interpersonal emotion regulation involves using diverse strategies to influence others’ emotions, commonly assessed with questionnaires. However, this method may be less effective for individuals with limited literacy or introspection skills. To address this, recent studies have adopted narrative-based approaches, though these require time-intensive qualitative analysis. Given the potential of artificial intelligence ({AI}) and large language models ({LLM}) for information classification, we evaluated the feasibility of using {AI} to categorize interpersonal emotion regulation strategies. We conducted two studies in which we compared {AI} performance against human coding in identifying regulation strategies from narrative data. In Study 1, with 2,824 responses, {ChatGPT} initially achieved Kappa values over .47. Refinements in prompts (i.e., coding instructions) led to improved consistency between {ChatGPT} and human coders (κ {\textgreater} .79). In Study 2, the refined prompts demonstrated comparable accuracy (κ {\textgreater} .76) when analyzing a new set of responses (N = 2090), using both {ChatGPT} and Claude. Additional evaluations of {LLMs}’ performance using different accuracy metrics pointed to notable variability in {LLM}’s capability when interpreting narratives across different emotions and regulatory strategies. These results point to the strengths and limitations of {LLMs} in classifying regulation strategies, and the importance of prompt engineering and validation. ({PsycInfo} Database Record (c) 2025 {APA}, all rights reserved)},
	journaltitle = {Emotion},
	author = {López-Pérez, Belén and Chen, Yuhui and Li, Xiuhui and Cheng, Shixing and Razavi, Pooya},
	date = {2025},
	note = {Place: {US}
Publisher: American Psychological Association},
}

@article{cheng_conceptual_2025,
	title = {The conceptual structure of human relationships across modern and historical cultures},
	rights = {2025 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-025-02122-8},
	doi = {10.1038/s41562-025-02122-8},
	abstract = {A defining characteristic of social complexity in Homo sapiens is the diversity of our relationships. We build connections of various types in our families, workplaces, neighbourhoods and online communities. How do we make sense of such complex systems of human relationships? The basic organization of relationships has long been studied in the social sciences, but no consensus has been reached. Here, by using online surveys, laboratory cognitive tasks and natural language processing in diverse modern cultures across the world (n = 20,427) and ancient cultures spanning 3,000 years of history, we examined universality and cultural variability in the ways that people conceptualize relationships. We discovered a universal representational space for relationship concepts, comprising five principal dimensions (formality, activeness, valence, exchange and equality) and three core categories (hostile, public and private relationships). Our work reveals the fundamental cognitive constructs and cultural principles of human relationship knowledge and advances our understanding of human sociality.},
	pages = {1--14},
	journaltitle = {Nature Human Behaviour},
	shortjournal = {Nat Hum Behav},
	author = {Cheng, Xi and Popal, Haroon and Wang, Huanqing and Hu, Renfen and Zang, Yinyin and Zhang, Mingzhe and Thornton, Mark A. and Ma, Yina and Cai, Huajian and Bi, Yanchao and Reilly, Jamie and Olson, Ingrid R. and Wang, Yin},
	urldate = {2025-03-31},
	date = {2025-03-13},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
}

@article{todorov_social_2015,
	title = {Social Attributions from Faces: Determinants, Consequences, Accuracy, and Functional Significance},
	volume = {66},
	issn = {0066-4308, 1545-2085},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev-psych-113011-143831},
	doi = {10.1146/annurev-psych-113011-143831},
	shorttitle = {Social Attributions from Faces},
	abstract = {Since the early twentieth century, psychologists have known that there is consensus in attributing social and personality characteristics from facial appearance. Recent studies have shown that surprisingly little time and effort are needed to arrive at this consensus. Here we review recent research on social attributions from faces. Section I outlines data-driven methods capable of identifying the perceptual basis of consensus in social attributions from faces (e.g., What makes a face look threatening?). Section {II} describes nonperceptual determinants of social attributions (e.g., person knowledge and incidental associations). Section {III} discusses evidence that attributions from faces predict important social outcomes in diverse domains (e.g., investment decisions and leader selection). In Section {IV}, we argue that the diagnostic validity of these attributions has been greatly overstated in the literature. In the final section, we offer an account of the functional significance of these attributions.},
	pages = {519--545},
	issue = {Volume 66, 2015},
	journaltitle = {Annual Review of Psychology},
	author = {Todorov, Alexander and Olivola, Christopher Y. and Dotsch, Ron and Mende-Siedlecki, Peter},
	urldate = {2025-03-31},
	date = {2015-01-03},
	langid = {english},
	note = {Publisher: Annual Reviews},
}

@article{xu_decoding_2024,
	title = {Decoding the Temporal Structures and Interactions of Multiple Face Dimensions Using Optically Pumped Magnetometer Magnetoencephalography ({OPM}-{MEG})},
	volume = {44},
	rights = {https://creativecommons.org/licenses/by-nc-sa/4.0/},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.2237-23.2024},
	doi = {10.1523/JNEUROSCI.2237-23.2024},
	abstract = {Humans possess a remarkable ability to rapidly access diverse information from others’ faces with just a brief glance, which is crucial for intricate social interactions. While previous studies using event-related potentials/fields have explored various face dimensions during this process, the interplay between these dimensions remains unclear. Here, by applying multivariate decoding analysis to neural signals recorded with optically pumped magnetometer magnetoencephalography, we systematically investigated the temporal interactions between invariant and variable aspects of face stimuli, including race, gender, age, and expression. First, our analysis revealed unique temporal structures for each face dimension with high test–retest reliability. Notably, expression and race exhibited a dominant and stably maintained temporal structure according to temporal generalization analysis. Further exploration into the mutual interactions among face dimensions uncovered age effects on gender and race, as well as expression effects on race, during the early stage (∼200–300 ms postface presentation). Additionally, we observed a relatively late effect of race on gender representation, peaking ∼350 ms after the stimulus onset. Taken together, our findings provide novel insights into the neural dynamics underlying the multidimensional aspects of face perception and illuminate the promising future of utilizing {OPM}-{MEG} for exploring higher-level human cognition.},
	pages = {e2237232024},
	number = {47},
	journaltitle = {The Journal of Neuroscience},
	shortjournal = {J. Neurosci.},
	author = {Xu, Wei and Lyu, Bingjiang and Ru, Xingyu and Li, Dongxu and Gu, Wenyu and Ma, Xiao and Zheng, Fufu and Li, Tingyue and Liao, Pan and Cheng, Hao and Yang, Rui and Song, Jingqi and Jin, Zeyu and Li, Congcong and He, Kaiyan and Gao, Jia-Hong},
	urldate = {2025-03-27},
	date = {2024-11-20},
	langid = {english},
	keywords = {{RSA}, dimension},
}

@article{sievers_deep_2024,
	title = {Deep social neuroscience: the promise and peril of using artificial neural networks to study the social brain},
	volume = {19},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1749-5016, 1749-5024},
	url = {https://academic.oup.com/scan/article/doi/10.1093/scan/nsae014/7604387},
	doi = {10.1093/scan/nsae014},
	shorttitle = {Deep social neuroscience},
	abstract = {This review offers an accessible primer to social neuroscientists interested in neural networks. It begins by providing an overview of key concepts in deep learning. It then discusses three ways neural networks can be useful to social neuroscientists: (i) building statistical models to predict behavior from brain activity; (ii) quantifying naturalistic stimuli and social interactions; and (iii) generating cognitive models of social brain function. These applications have the potential to enhance the clinical value of neuroimaging and improve the generalizability of social neuroscience research. We also discuss the significant practical challenges, theoretical limitations and ethical issues faced by deep learning. If the field can successfully navigate these hazards, we believe that artificial neural networks may prove indispensable for the next stage of the field’s development: deep social neuroscience.},
	pages = {nsae014},
	number = {1},
	journaltitle = {Social Cognitive and Affective Neuroscience},
	author = {Sievers, Beau and Thornton, Mark A},
	urldate = {2024-11-23},
	date = {2024-02-27},
	langid = {english},
}

@article{awad_moral_2018,
	title = {The Moral Machine experiment},
	volume = {563},
	rights = {2018 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-018-0637-6},
	doi = {10.1038/s41586-018-0637-6},
	abstract = {With the rapid development of artificial intelligence have come concerns about how machines will make moral decisions, and the major challenge of quantifying societal expectations about the ethical principles that should guide machine behaviour. To address this challenge, we deployed the Moral Machine, an online experimental platform designed to explore the moral dilemmas faced by autonomous vehicles. This platform gathered 40 million decisions in ten languages from millions of people in 233 countries and territories. Here we describe the results of this experiment. First, we summarize global moral preferences. Second, we document individual variations in preferences, based on respondents’ demographics. Third, we report cross-cultural ethical variation, and uncover three major clusters of countries. Fourth, we show that these differences correlate with modern institutions and deep cultural traits. We discuss how these preferences can contribute to developing global, socially acceptable principles for machine ethics. All data used in this article are publicly available.},
	pages = {59--64},
	number = {7729},
	journaltitle = {Nature},
	author = {Awad, Edmond and Dsouza, Sohan and Kim, Richard and Schulz, Jonathan and Henrich, Joseph and Shariff, Azim and Bonnefon, Jean-François and Rahwan, Iyad},
	urldate = {2024-11-13},
	date = {2018-11},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
}

@article{almaatouq_beyond_2024,
	title = {Beyond playing 20 questions with nature: Integrative experiment design in the social and behavioral sciences},
	volume = {47},
	issn = {0140-525X, 1469-1825},
	url = {https://www.cambridge.org/core/product/identifier/S0140525X22002874/type/journal_article},
	doi = {10.1017/S0140525X22002874},
	shorttitle = {Beyond playing 20 questions with nature},
	abstract = {The dominant paradigm of experiments in the social and behavioral sciences views an experiment as a test of a theory, where the theory is assumed to generalize beyond the experiment’s specific conditions. According to this view, which Alan Newell once characterized as “playing twenty questions with nature,” theory is advanced one experiment at a time, and the integration of disparate findings is assumed to happen via the scientific publishing process. In this article, we argue that the process of integration is at best inefficient, and at worst it does not, in fact, occur. We further show that the challenge of integration cannot be adequately addressed by recently proposed reforms that focus on the reliability and replicability of individual findings, nor simply by conducting more or larger experiments. Rather, the problem arises from the imprecise nature of social and behavioral theories and, consequently, a lack of commensurability across experiments conducted under different conditions. Therefore, researchers must fundamentally rethink how they design experiments and how the experiments relate to theory. We specifically describe an alternative framework, integrative experiment design, which intrinsically promotes commensurability and continuous integration of knowledge. In this paradigm, researchers explicitly map the design space of possible experiments associated with a given research question, embracing many potentially relevant theories rather than focusing on just one. Researchers then iteratively generate theories and test them with experiments explicitly sampled from the design space, allowing results to be integrated across experiments. Given recent methodological and technological developments, we conclude that this approach is feasible and would generate more-reliable, more-cumulative empirical and theoretical knowledge than the current paradigm – and with far greater efficiency.},
	pages = {e33},
	journaltitle = {Behavioral and Brain Sciences},
	shortjournal = {Behav Brain Sci},
	author = {Almaatouq, Abdullah and Griffiths, Thomas L. and Suchow, Jordan W. and Whiting, Mark E. and Evans, James and Watts, Duncan J.},
	urldate = {2024-10-24},
	date = {2024},
	langid = {english},
}

@misc{radford_learning_2021,
	title = {Learning Transferable Visual Models From Natural Language Supervision},
	url = {http://arxiv.org/abs/2103.00020},
	doi = {10.48550/arXiv.2103.00020},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn {SOTA} image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as {OCR}, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original {ResNet}-50 on {ImageNet} zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/{OpenAI}/{CLIP}.},
	number = {{arXiv}:2103.00020},
	publisher = {{arXiv}},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	urldate = {2025-03-13},
	date = {2021-02-26},
	langid = {american},
	eprinttype = {arxiv},
	eprint = {2103.00020 [cs]},
}

@article{nili_toolbox_2014,
	title = {A Toolbox for Representational Similarity Analysis},
	volume = {10},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003553},
	doi = {10.1371/journal.pcbi.1003553},
	abstract = {Neuronal population codes are increasingly being investigated with multivariate pattern-information analyses. A key challenge is to use measured brain-activity patterns to test computational models of brain information processing. One approach to this problem is representational similarity analysis ({RSA}), which characterizes a representation in a brain or computational model by the distance matrix of the response patterns elicited by a set of stimuli. The representational distance matrix encapsulates what distinctions between stimuli are emphasized and what distinctions are de-emphasized in the representation. A model is tested by comparing the representational distance matrix it predicts to that of a measured brain region. {RSA} also enables us to compare representations between stages of processing within a given brain or model, between brain and behavioral data, and between individuals and species. Here, we introduce a Matlab toolbox for {RSA}. The toolbox supports an analysis approach that is simultaneously data- and hypothesis-driven. It is designed to help integrate a wide range of computational models into the analysis of multichannel brain-activity measurements as provided by modern functional imaging and neuronal recording techniques. Tools for visualization and inference enable the user to relate sets of models to sets of brain regions and to statistically test and compare the models using nonparametric inference methods. The toolbox supports searchlight-based {RSA}, to continuously map a measured brain volume in search of a neuronal population code with a specific geometry. Finally, we introduce the linear-discriminant t value as a measure of representational discriminability that bridges the gap between linear decoding analyses and {RSA}. In order to demonstrate the capabilities of the toolbox, we apply it to both simulated and real {fMRI} data. The key functions are equally applicable to other modalities of brain-activity measurement. The toolbox is freely available to the community under an open-source license agreement (http://www.mrc-cbu.cam.ac.uk/methods-and-resources/toolboxes/license/).},
	pages = {e1003553},
	number = {4},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLOS} Computational Biology},
	author = {Nili, Hamed and Wingfield, Cai and Walther, Alexander and Su, Li and Marslen-Wilson, William and Kriegeskorte, Nikolaus},
	urldate = {2025-03-13},
	date = {2014-04-17},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {methodology},
}

@misc{monfort_moments_2019,
	title = {Moments in Time Dataset: one million videos for event understanding},
	url = {http://arxiv.org/abs/1801.03150},
	doi = {10.48550/arXiv.1801.03150},
	shorttitle = {Moments in Time Dataset},
	abstract = {We present the Moments in Time Dataset, a large-scale human-annotated collection of one million short videos corresponding to dynamic events unfolding within three seconds. Modeling the spatial-audio-temporal dynamics even for actions occurring in 3 second videos poses many challenges: meaningful events do not include only people, but also objects, animals, and natural phenomena; visual and auditory events can be symmetrical in time ("opening" is "closing" in reverse), and either transient or sustained. We describe the annotation process of our dataset (each video is tagged with one action or activity label among 339 different classes), analyze its scale and diversity in comparison to other large-scale video datasets for action recognition, and report results of several baseline models addressing separately, and jointly, three modalities: spatial, temporal and auditory. The Moments in Time dataset, designed to have a large coverage and diversity of events in both visual and auditory modalities, can serve as a new challenge to develop models that scale to the level of complexity and abstract reasoning that a human processes on a daily basis.},
	number = {{arXiv}:1801.03150},
	publisher = {{arXiv}},
	author = {Monfort, Mathew and Andonian, Alex and Zhou, Bolei and Ramakrishnan, Kandan and Bargal, Sarah Adel and Yan, Tom and Brown, Lisa and Fan, Quanfu and Gutfruend, Dan and Vondrick, Carl and Oliva, Aude},
	urldate = {2025-03-14},
	date = {2019-02-16},
	eprinttype = {arxiv},
	eprint = {1801.03150 [cs]},
}

@misc{carvalho_naturalistic_2025,
	title = {Naturalistic Computational Cognitive Science: Towards generalizable models and theories that capture the full range of natural behavior},
	url = {http://arxiv.org/abs/2502.20349},
	doi = {10.48550/arXiv.2502.20349},
	shorttitle = {Naturalistic Computational Cognitive Science},
	abstract = {Artificial Intelligence increasingly pursues large, complex models that perform many tasks within increasingly realistic domains. How, if at all, should these developments in {AI} influence cognitive science? We argue that progress in {AI} offers timely opportunities for cognitive science to embrace experiments with increasingly naturalistic stimuli, tasks, and behaviors; and computational models that can accommodate these changes. We first review a growing body of research spanning neuroscience, cognitive science, and {AI} that suggests that incorporating a broader range of naturalistic experimental paradigms (and models that accommodate them) may be necessary to resolve some aspects of natural intelligence and ensure that our theories generalize. We then suggest that integrating recent progress in {AI} and cognitive science will enable us to engage with more naturalistic phenomena without giving up experimental control or the pursuit of theoretically grounded understanding. We offer practical guidance on how methodological practices can contribute to cumulative progress in naturalistic computational cognitive science, and illustrate a path towards building computational models that solve the real problems of natural cognition - together with a reductive understanding of the processes and principles by which they do so.},
	number = {{arXiv}:2502.20349},
	publisher = {{arXiv}},
	author = {Carvalho, Wilka and Lampinen, Andrew},
	urldate = {2025-02-28},
	date = {2025-02-27},
	langid = {american},
	eprinttype = {arxiv},
	eprint = {2502.20349 [q-bio]},
	keywords = {note},
}

@misc{lu_animate_2025,
	title = {Animate Your Thoughts: Decoupled Reconstruction of Dynamic Natural Vision from Slow Brain Activity},
	url = {http://arxiv.org/abs/2405.03280},
	doi = {10.48550/arXiv.2405.03280},
	shorttitle = {Animate Your Thoughts},
	abstract = {Reconstructing human dynamic vision from brain activity is a challenging task with great scientific significance. Although prior video reconstruction methods have made substantial progress, they still suffer from several limitations, including: (1) difficulty in simultaneously reconciling semantic (e.g. categorical descriptions), structure (e.g. size and color), and consistent motion information (e.g. order of frames); (2) low temporal resolution of {fMRI}, which poses a challenge in decoding multiple frames of video dynamics from a single {fMRI} frame; (3) reliance on video generation models, which introduces ambiguity regarding whether the dynamics observed in the reconstructed videos are genuinely derived from {fMRI} data or are hallucinations from generative model. To overcome these limitations, we propose a two-stage model named Mind-Animator. During the {fMRI}-to-feature stage, we decouple semantic, structure, and motion features from {fMRI}. Specifically, we employ {fMRI}-vision-language tri-modal contrastive learning to decode semantic feature from {fMRI} and design a sparse causal attention mechanism for decoding multi-frame video motion features through a next-frame-prediction task. In the feature-to-video stage, these features are integrated into videos using an inflated Stable Diffusion, effectively eliminating external video data interference. Extensive experiments on multiple video-{fMRI} datasets demonstrate that our model achieves state-of-the-art performance. Comprehensive visualization analyses further elucidate the interpretability of our model from a neurobiological perspective. Project page: https://mind-animator-design.github.io/.},
	number = {{arXiv}:2405.03280},
	publisher = {{arXiv}},
	author = {Lu, Yizhuo and Du, Changde and Wang, Chong and Zhu, Xuanliu and Jiang, Liuyun and Li, Xujin and He, Huiguang},
	urldate = {2025-03-31},
	date = {2025-02-19},
	eprinttype = {arxiv},
	eprint = {2405.03280 [cs]},
}

@article{ye_generative_2025,
	title = {Generative language reconstruction from brain recordings},
	volume = {8},
	issn = {2399-3642},
	url = {https://www.nature.com/articles/s42003-025-07731-7},
	doi = {10.1038/s42003-025-07731-7},
	pages = {346},
	number = {1},
	journaltitle = {Communications Biology},
	shortjournal = {Commun Biol},
	author = {Ye, Ziyi and Ai, Qingyao and Liu, Yiqun and De Rijke, Maarten and Zhang, Min and Lioma, Christina and Ruotsalo, Tuukka},
	urldate = {2025-03-31},
	date = {2025-03-01},
	langid = {english},
}

@article{grand_semantic_2022,
	title = {Semantic projection recovers rich human knowledge of multiple object features from word embeddings},
	volume = {6},
	rights = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-022-01316-8},
	doi = {10.1038/s41562-022-01316-8},
	abstract = {How is knowledge about word meaning represented in the mental lexicon? Current computational models infer word meanings from lexical co-occurrence patterns. They learn to represent words as vectors in a multidimensional space, wherein words that are used in more similar linguistic contexts—that is, are more semantically related—are located closer together. However, whereas inter-word proximity captures only overall relatedness, human judgements are highly context dependent. For example, dolphins and alligators are similar in size but differ in dangerousness. Here, we use a domain-general method to extract context-dependent relationships from word embeddings: ‘semantic projection’ of word-vectors onto lines that represent features such as size (the line connecting the words ‘small’ and ‘big’) or danger (‘safe’ to ‘dangerous’), analogous to ‘mental scales’. This method recovers human judgements across various object categories and properties. Thus, the geometry of word embeddings explicitly represents a wealth of context-dependent world knowledge.},
	pages = {975--987},
	number = {7},
	journaltitle = {Nature Human Behaviour},
	shortjournal = {Nat Hum Behav},
	author = {Grand, Gabriel and Blank, Idan Asher and Pereira, Francisco and Fedorenko, Evelina},
	urldate = {2025-03-31},
	date = {2022-07},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
}

@article{cutler_archetypal_1994,
	title = {Archetypal Analysis},
	volume = {36},
	issn = {0040-1706},
	url = {https://www.tandfonline.com/doi/abs/10.1080/00401706.1994.10485840},
	doi = {10.1080/00401706.1994.10485840},
	abstract = {Archetypal analysis represents each individual in a data set as a mixture of individuals of pure type or archetypes. The archetypes themselves are restricted to being mixtures of the individuals in the data set. Archetypes are selected by minimizing the squared error in representing each individual as a mixture of archetypes. The usefulness of archetypal analysis is illustrated on several data sets. Computing the archetypes is a nonlinear least squares problem, which is solved using an alternating minimizing algorithm.},
	pages = {338--347},
	number = {4},
	journaltitle = {Technometrics},
	author = {Cutler, Adele and and Breiman, Leo},
	urldate = {2025-03-31},
	date = {1994-11-01},
	note = {Publisher: {ASA} Website
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00401706.1994.10485840},
}

@misc{fel_archetypal_2025,
	title = {Archetypal {SAE}: Adaptive and Stable Dictionary Learning for Concept Extraction in Large Vision Models},
	url = {http://arxiv.org/abs/2502.12892},
	doi = {10.48550/arXiv.2502.12892},
	shorttitle = {Archetypal {SAE}},
	abstract = {Sparse Autoencoders ({SAEs}) have emerged as a powerful framework for machine learning interpretability, enabling the unsupervised decomposition of model representations into a dictionary of abstract, human-interpretable concepts. However, we reveal a fundamental limitation: existing {SAEs} exhibit severe instability, as identical models trained on similar datasets can produce sharply different dictionaries, undermining their reliability as an interpretability tool. To address this issue, we draw inspiration from the Archetypal Analysis framework introduced by Cutler \& Breiman (1994) and present Archetypal {SAEs} (A-{SAE}), wherein dictionary atoms are constrained to the convex hull of data. This geometric anchoring significantly enhances the stability of inferred dictionaries, and their mildly relaxed variants {RA}-{SAEs} further match state-of-the-art reconstruction abilities. To rigorously assess dictionary quality learned by {SAEs}, we introduce two new benchmarks that test (i) plausibility, if dictionaries recover "true" classification directions and (ii) identifiability, if dictionaries disentangle synthetic concept mixtures. Across all evaluations, {RA}-{SAEs} consistently yield more structured representations while uncovering novel, semantically meaningful concepts in large-scale vision models.},
	number = {{arXiv}:2502.12892},
	publisher = {{arXiv}},
	author = {Fel, Thomas and Lubana, Ekdeep Singh and Prince, Jacob S. and Kowal, Matthew and Boutin, Victor and Papadimitriou, Isabel and Wang, Binxu and Wattenberg, Martin and Ba, Demba and Konkle, Talia},
	urldate = {2025-03-31},
	date = {2025-02-18},
	eprinttype = {arxiv},
	eprint = {2502.12892 [cs]},
}

@article{lee_masson_multidimensional_2024,
	title = {Multidimensional neural representations of social features during movie viewing},
	volume = {19},
	rights = {https://creativecommons.org/licenses/by-nc/4.0/},
	issn = {1749-5016, 1749-5024},
	url = {https://academic.oup.com/scan/article/doi/10.1093/scan/nsae030/7667785},
	doi = {10.1093/scan/nsae030},
	abstract = {The social world is dynamic and contextually embedded. Yet, most studies utilize simple stimuli that do not capture the complexity of everyday social episodes. To address this, we implemented a movie viewing paradigm and investigated how everyday social episodes are processed in the brain. Participants watched one of two movies during an {MRI} scan. Neural patterns from brain regions involved in social perception, mentalization, action observation and sensory processing were extracted. Representational similarity analysis results revealed that several labeled social features (including social interaction, mentalization, the actions of others, characters talking about themselves, talking about others and talking about objects) were represented in the superior temporal gyrus ({STG}) and middle temporal gyrus ({MTG}). The mentalization feature was also represented throughout the theory of mind network, and characters talking about others engaged the temporoparietal junction ({TPJ}), suggesting that listeners may spontaneously infer the mental state of those being talked about. In contrast, we did not observe the action representations in the frontoparietal regions of the action observation network. The current findings indicate that {STG} and {MTG} serve as key regions for social processing, and that listening to characters talk about others elicits spontaneous mental state inference in {TPJ} during natural movie viewing.},
	pages = {nsae030},
	number = {1},
	journaltitle = {Social Cognitive and Affective Neuroscience},
	author = {Lee Masson, Haemy and Chang, Lucy and Isik, Leyla},
	urldate = {2025-02-25},
	date = {2024-05-27},
	langid = {english},
	keywords = {note},
}

@article{mcmahon_hierarchical_2023,
	title = {Hierarchical organization of social action features along the lateral visual pathway},
	volume = {33},
	issn = {09609822},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0960982223013738},
	doi = {10.1016/j.cub.2023.10.015},
	abstract = {Recent theoretical work has argued that in addition to the classical ventral (what) and dorsal (where/how) visual streams, there is a third visual stream on the lateral surface of the brain specialized for processing social information. Like visual representations in the ventral and dorsal streams, representations in the lateral stream are thought to be hierarchically organized. However, no prior studies have comprehensively investigated the organization of naturalistic, social visual content in the lateral stream. To address this question, we curated a naturalistic stimulus set of 250 3-s videos of two people engaged in everyday actions. Each clip was richly annotated for its low-level visual features, mid-level scene and object properties, visual social primitives (including the distance between people and the extent to which they were facing), and high-level information about social interactions and affective content. Using a condition-rich {fMRI} experiment and a within-subject encoding model approach, we found that low-level visual features are represented in early visual cortex ({EVC}) and middle temporal ({MT}) area, mid-level visual social features in extrastriate body area ({EBA}) and lateral occipital complex ({LOC}), and high-level social interaction information along the superior temporal sulcus ({STS}). Communicative interactions, in particular, explained unique variance in regions of the {STS} after accounting for variance explained by all other labeled features. Taken together, these results provide support for representation of increasingly abstract social visual content—consistent with hierarchical organization—along the lateral visual stream and suggest that recognizing communicative actions may be a key computational goal of the lateral visual pathway.},
	pages = {5035--5047.e8},
	number = {23},
	journaltitle = {Current Biology},
	shortjournal = {Current Biology},
	author = {{McMahon}, Emalie and Bonner, Michael F. and Isik, Leyla},
	urldate = {2025-01-26},
	date = {2023-12},
	langid = {english},
	keywords = {Expand, note},
}

@article{defossez_decoding_2023,
	title = {Decoding speech perception from non-invasive brain recordings},
	volume = {5},
	rights = {2023 The Author(s)},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-023-00714-5},
	doi = {10.1038/s42256-023-00714-5},
	abstract = {Decoding speech from brain activity is a long-awaited goal in both healthcare and neuroscience. Invasive devices have recently led to major milestones in this regard: deep-learning algorithms trained on intracranial recordings can now start to decode elementary linguistic features such as letters, words and audio-spectrograms. However, extending this approach to natural speech and non-invasive brain recordings remains a major challenge. Here we introduce a model trained with contrastive learning to decode self-supervised representations of perceived speech from the non-invasive recordings of a large cohort of healthy individuals. To evaluate this approach, we curate and integrate four public datasets, encompassing 175 volunteers recorded with magneto-encephalography or electro-encephalography while they listened to short stories and isolated sentences. The results show that our model can identify, from 3 seconds of magneto-encephalography signals, the corresponding speech segment with up to 41\% accuracy out of more than 1,000 distinct possibilities on average across participants, and with up to 80\% in the best participants—a performance that allows the decoding of words and phrases absent from the training set. The comparison of our model with a variety of baselines highlights the importance of a contrastive objective, pretrained representations of speech and a common convolutional architecture simultaneously trained across multiple participants. Finally, the analysis of the decoder’s predictions suggests that they primarily depend on lexical and contextual semantic representations. Overall, this effective decoding of perceived speech from non-invasive recordings delineates a promising path to decode language from brain activity, without putting patients at risk of brain surgery.},
	pages = {1097--1107},
	number = {10},
	journaltitle = {Nature Machine Intelligence},
	shortjournal = {Nat Mach Intell},
	author = {Défossez, Alexandre and Caucheteux, Charlotte and Rapin, Jérémy and Kabeli, Ori and King, Jean-Rémi},
	urldate = {2025-03-19},
	date = {2023-10},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
}

@article{tang_semantic_2023,
	title = {Semantic reconstruction of continuous language from non-invasive brain recordings},
	volume = {26},
	rights = {2023 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/s41593-023-01304-9},
	doi = {10.1038/s41593-023-01304-9},
	abstract = {A brain–computer interface that decodes continuous language from non-invasive recordings would have many scientific and practical applications. Currently, however, non-invasive language decoders can only identify stimuli from among a small set of words or phrases. Here we introduce a non-invasive decoder that reconstructs continuous language from cortical semantic representations recorded using functional magnetic resonance imaging ({fMRI}). Given novel brain recordings, this decoder generates intelligible word sequences that recover the meaning of perceived speech, imagined speech and even silent videos, demonstrating that a single decoder can be applied to a range of tasks. We tested the decoder across cortex and found that continuous language can be separately decoded from multiple regions. As brain–computer interfaces should respect mental privacy, we tested whether successful decoding requires subject cooperation and found that subject cooperation is required both to train and to apply the decoder. Our findings demonstrate the viability of non-invasive language brain–computer interfaces.},
	pages = {858--866},
	number = {5},
	journaltitle = {Nature Neuroscience},
	shortjournal = {Nat Neurosci},
	author = {Tang, Jerry and {LeBel}, Amanda and Jain, Shailee and Huth, Alexander G.},
	urldate = {2025-03-19},
	date = {2023-05},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
}
